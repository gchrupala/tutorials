{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A machine learning project workflow\n",
      "-----------------------------------\n",
      "\n",
      "Most machine learning projects need to follow several common steps. In this notebook we illustrate how to load data, preprocess it, and test a couple of models on it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "from sklearn.datasets import load_files\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from scipy.sparse import csr_matrix, hstack\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.metrics import zero_one_loss"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we need to load tabular data with mixed categorical and numerical data, see the example on BlackBoard https://edubb.uvt.nl/bbcswebdav/pid-1196690-dt-content-rid-3444809_1/xid-3444809_1\n",
      "\n",
      "Here we show how to combine text and numerical features.\n",
      "\n",
      "If you need to load text files, we can use ``laod_files`` from sklearn.datasets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = load_files('/home/gchrupala/repos/rsml/data/aclImdb/train/', categories=['pos','neg'], shuffle=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll use the first 5000 examples for development"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "texts_de  = data['data'][:5000]\n",
      "texts_tr  = data['data'][5000:]\n",
      "target_de = data['target'][:5000]\n",
      "target_tr = data['target'][5000:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We get back a dictionary-like object with several keys, including ``target`` which has the vector of targets, and ``data`` which is a list of texts. We will extract word counts from the texts using ``CountVectorizer``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(min_df=3) # Ignore words that occur in less than 3 document\n",
      "text_tr_counts = vectorizer.fit_transform(texts_tr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also want another feature which counts the total length of the text in characters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def text_length(texts):\n",
      "    return numpy.array([ len(text) for text in texts ], dtype='float')\n",
      "\n",
      "text_tr_lens = text_length(texts_tr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This feature is on a different scale from the word counts.\n",
      "\n",
      "We could standardize all the features -- i.e. subtract the mean and divide by standard deviation, \n",
      "but this is not a good idea for word-counts: they are stored in a sparse matrix, and if we tried \n",
      "to subtract the mean, sparsity would be lost and our matrix would become dense, and huge. So we'll\n",
      "only standardize the text length feature -- this will at least make it a bit more similar in scale \n",
      "to the word counts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scaler = StandardScaler()\n",
      "text_tr_lens_scaled = scaler.fit_transform(text_tr_lens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_tr_lens_scaled"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "array([ 0.57289657,  1.45574235, -0.93231571, ..., -0.25029747,\n",
        "       -0.33182151,  0.72302012])"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_tr_counts[0].todense()[0,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "matrix([[0, 0, 0, ..., 0, 0, 0]])"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can put the text length feature together with the word counts. We need to convert the vector of lengths to a sparse format, \n",
      "and stack it as another column of the counts matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def append_col(counts, lens):\n",
      "    return hstack([counts, numpy.transpose(csr_matrix(lens))])\n",
      "\n",
      "train_inputs = append_col(text_tr_counts, text_tr_lens_scaled)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are now ready to train a model. Let's use SGDClassifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = SGDClassifier(n_iter=20, loss='log')\n",
      "classifier.fit(train_inputs, target_tr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "SGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,\n",
        "       fit_intercept=True, l1_ratio=0.15, learning_rate='optimal',\n",
        "       loss='log', n_iter=20, n_jobs=1, penalty='l2', power_t=0.5,\n",
        "       random_state=None, rho=None, shuffle=False, verbose=0,\n",
        "       warm_start=False)"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to evaluate the model, we need to transform the test or dev data in the same way we did the training data. \n",
      "We package the steps in the function ``preprocess``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def preprocess(vectorizer, scaler, texts):\n",
      "    counts = vectorizer.transform(texts_de)\n",
      "    lens = scaler.transform(text_length(texts_de))\n",
      "    return append_col(counts, lens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dev_inputs  = preprocess(vectorizer, scaler, texts_de)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can check how the model is doing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print zero_one_loss(target_de, classifier.predict(dev_inputs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.1352\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could make the whole thing a bit less cumbersome by using the ``sklearn.pipeline`` module."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}