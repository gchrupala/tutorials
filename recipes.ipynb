{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Recipes "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook contains a few recipes for loading and manipulating data. These techniques are useful \n",
      "in machine learning projects."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Loading mixed data\n",
      "\n",
      "Many datasets contain heterogeneous features: numeric, categorical, and text. However, the machine learning toolkit \n",
      "scikit learn expects its input to be numeric numpy arrays, and so there is some data manipulation that needs to be done.\n",
      "\n",
      "For this section you'll need these files:\n",
      "\n",
      "- [airline-train-features.csv](airline-train-features.csv)\n",
      "- [airline-test-features.csv](airline-test-features.csv)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The most basic way of loading data from a CSV file is to use the csv module. It allows you to read rows one by one and\n",
      "store them in a list of dictionaries mapping feature names to feature values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "import numpy\n",
      "def read_data(path):\n",
      "    rows = []\n",
      "    with open(path) as file:\n",
      "        reader = csv.DictReader(file)\n",
      "        for row in reader:\n",
      "            rows.append(row)\n",
      "        return rows"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_train = read_data(\"airline-train-features.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_train[0:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[{'Id': '0',\n",
        "  'airline': 'Virgin America',\n",
        "  'location': '',\n",
        "  'retweet_count': '0',\n",
        "  'text': '@VirginAmerica What @dhepburn said.',\n",
        "  'timestamp': '2015-02-24 11:35:52 -0800',\n",
        "  'username': 'cairdin'},\n",
        " {'Id': '1',\n",
        "  'airline': 'Virgin America',\n",
        "  'location': '',\n",
        "  'retweet_count': '0',\n",
        "  'text': \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
        "  'timestamp': '2015-02-24 11:15:59 -0800',\n",
        "  'username': 'jnardino'},\n",
        " {'Id': '2',\n",
        "  'airline': 'Virgin America',\n",
        "  'location': 'Lets Play',\n",
        "  'retweet_count': '0',\n",
        "  'text': \"@VirginAmerica I didn't today... Must mean I need to take another trip!\",\n",
        "  'timestamp': '2015-02-24 11:15:48 -0800',\n",
        "  'username': 'yvonnalynn'}]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Extracting features\n",
      "We will now define a pair of functions which will extract features from the data and stores them in a sparse array. The first function `features_fit_transform` should be used on training data. It returns the feature array, and also the information necessary \n",
      "to extract the same features from new data, such as your test set. This information is for example mappings from feature names to fueature indices, or the settings of the ngram extractor. This information is stored in a dictionary names `transformer` in the function definition below. \n",
      "\n",
      "Our second function `features_transform` should be used on development or test data. It takes the transformer dictionary as one input, and uses it to extract features from the given data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.sparse\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "def features_fit_transform(data):\n",
      "    \"\"\"Returns features extracted from data, and objects necessary to extract same \n",
      "    features from new data.\n",
      "    \n",
      "    `data` is a list of dictionaries which map feature names to feature values.\n",
      "    \"\"\"\n",
      "    # In this example we will extract three sets of features:\n",
      "    # airline as a categorical\n",
      "    # location as a categorical \n",
      "    # retweet_count as a numerical\n",
      "    # text as character ngrams of size 1 to 3\n",
      "    # You can adapt this function to extract additional features\n",
      "    \n",
      "    transformer = {}\n",
      "    # First we convert categorical features to one-hot encoding using \n",
      "    #    http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer\n",
      "    categorical = [ { 'airline': row['airline'], 'location': row['location'] } \n",
      "                   for row in data ]\n",
      "    transformer['categorical'] = DictVectorizer(sparse=True)\n",
      "    X_categorical = transformer['categorical'].fit_transform(categorical)\n",
      "    \n",
      "    # Convert retweet_count to a numerical (int) 2D array\n",
      "    X_numerical = numpy.array([ [row['retweet_count']] for row in data ], dtype='int')\n",
      "\n",
      "    # Extract character ngrams from text using \n",
      "    #   http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
      "    # CountVectorizer also can extract word ngrams, and has various other options for extracting text features.\n",
      "    transformer['text'] = CountVectorizer(analyzer='char', ngram_range=(1,3))\n",
      "    X_text = transformer['text'].fit_transform([row['text'] for row in data ])\n",
      "    \n",
      "    # Finally we concatenate these array columnwise using the hstack function, \n",
      "    # so that all the features are in a single sparse array\n",
      "    X = scipy.sparse.hstack([X_categorical, X_numerical, X_text])\n",
      "    # Return the extracted features together with the transformer object.\n",
      "    return (X, transformer)\n",
      "\n",
      "    \n",
      "def features_transform(transformer, data):\n",
      "    \"\"\"Returns features extracted from data, reusing the given transformer object.\"\"\"\n",
      "    categorical = [ { 'airline': row['airline'], 'location': row['location'] } \n",
      "                   for row in data ]\n",
      "    X_categorical = transformer['categorical'].transform(categorical)\n",
      "    X_numerical = numpy.array([ [row['retweet_count']] for row in data ], dtype='int')\n",
      "    X_text = transformer['text'].transform([row['text'] for row in data ])\n",
      "    X = scipy.sparse.hstack([X_categorical, X_numerical, X_text])\n",
      "    return X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, T = features_fit_transform(data_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_test = read_data(\"airline-test-features.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test = features_transform(T, data_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check the number of rows and columns in out data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "(11640, 31476)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "(3000, 31476)"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The number of columns matches betweet train and test set: good."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Loading labels\n",
      "\n",
      "We'll define a function which reads the CSV file with the labels and returns a list of labels.\n",
      "\n",
      "You'll need the files:\n",
      "\n",
      "- [airline-train-label.csv](airline-train-label.csv)\n",
      "- [airline-test-label.csv](airline-test-label.csv)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_label(f):\n",
      "    result = []\n",
      "    with open(f) as file:\n",
      "        reader = csv.reader(file)\n",
      "        for row in reader:\n",
      "            # skip the Id field\n",
      "            result.append(row[1])\n",
      "    # The first item is the header, we'll skip it\n",
      "    return result[1:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_train = read_label(\"airline-train-label.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_train[0:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "['neutral', 'positive', 'neutral']"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_test = read_label(\"airline-test-label.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are now ready to train a classifier on this data, and test its accuracy."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature transformations\n",
      "\n",
      "In this section we'll practice transforming features. We'll be using the iris dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split into training and test data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.33, random_state=42)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Standardization\n",
      "\n",
      "One of the most commonly used and useful transforms is to subtract the mean from each feature and divide by standard deviation.\n",
      "We will store the means and stdevs from training data in order to reuse them on new data (such as the test set). \n",
      "This is done using the familiar scikit-learn idiom of initializing a transformer object, fitting it \n",
      "on the training data while at the same time transforming it, and then transforming the test data.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler = StandardScaler()\n",
      "X_train_scaled = scaler.fit_transform(X_train)\n",
      "X_test_scaled = scaler.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check the means and standard deviations of the variables in the original and transformed training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# axis=0 computes function for each column\n",
      "print(numpy.mean(X_train, axis=0))\n",
      "print(numpy.std(X_train, axis=0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 5.815  3.006  3.817  1.218]\n",
        "[ 0.83118891  0.41396135  1.72609704  0.74489999]\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(numpy.mean(X_train_scaled, axis=0))\n",
      "print(numpy.std(X_train_scaled, axis=0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  1.70530257e-15  -1.62267769e-15  -3.93018951e-16  -2.70894418e-16]\n",
        "[ 1.  1.  1.  1.]\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks good. We could now use the scaled training data to train a learner, and the apply it to the scaled test set. \n",
      "\n",
      "**Try doing this on your own.**\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Dimensionality reduction\n",
      "\n",
      "In some cases, especially when we have a dense array with a very large number of columns, it can\n",
      "be useful to apply dimensionality reduction to our dataset. As an example, we'll use [Principal Components Analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn-decomposition-pca).\n",
      "\n",
      "The steps to follow are similar to those we used in standardization:\n",
      "\n",
      "- initialize PCA object, choosing options as appropriate\n",
      "- use the method `.fit_transform` to fit the PCA and simultaneously transform the training data\n",
      "- use the method `.transform` to transform new data (e.g. development or test set), reusing the PCA object\n",
      "\n",
      "**Try doing it on your own.**\n",
      "  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now use the reduced training data to train a learner, and the apply it to the reduced test set. \n",
      "\n",
      "**Try doing this on your own.**"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}